# Comprehensive AI-Augmented Workflow Guide

## **Ship Fast with Multi-Agent Power** - 28 Agents + Orchestration + Superpowers

**Version:** 1.0.0 **Date:** 2025-01-09 **Project:** Press On Ventures VC Fund
Modeling Platform **Purpose:** Practical guide for AI-augmented development with
28 specialized agents, multi-agent orchestration, coding pairs, and Superpowers
skills

---

## Table of Contents

- [Part I: Quick Start](#part-i-quick-start)
- [Part II: The 28 Specialized Agents](#part-ii-the-28-specialized-agents)
- [Part III: Multi-Agent Orchestration Patterns](#part-iii-multi-agent-orchestration-patterns)
- [Part IV: Coding Pairs Workflows](#part-iv-coding-pairs-workflows)
- [Part V: Superpowers Skills Integration](#part-v-superpowers-skills-integration)
- [Part VI: Logic & Thinking Skills](#part-vi-logic--thinking-skills)
- [Part VII: Quality Gates & Anti-Patterns](#part-vii-quality-gates--anti-patterns)
- [Part VIII: Memory & Knowledge Management](#part-viii-memory--knowledge-management)
- [Part IX: Troubleshooting & Debugging](#part-ix-troubleshooting--debugging)
- [Part X: Quick Reference & Appendices](#part-x-quick-reference--appendices)

---

## Part I: Quick Start

### What This System Does

This is an **AI-augmented development platform** combining:

- **28 specialized agents** for domain-specific tasks
- **Multi-agent orchestration** (parallel, sequential, hybrid execution)
- **Superpowers skills** (28 auto-activating workflows)
- **Extended thinking** (deep reasoning for complex problems)
- **4-layer quality gates** (anti-pattern prevention)

**Proven Results:**

- **87-91% time savings** on parallelizable tasks (documentation, batch
  operations)
- **30-50% savings** on sequential workflows with quality gates
- **50-75% savings** on hybrid pipelines (mixed parallel/sequential)
- **Real example:** Generated 2,400+ lines of documentation in 45 minutes using
  8 agents in parallel (vs 5.5 hours sequential)

### The 5 Commands You Need

```bash
/test-smart      # Intelligent test selection (~30s vs 5min full suite)
/fix-auto        # Auto-fix lint, types, simple test failures
/deploy-check    # 8-phase pre-deployment validation
/log-change      # Document changes in CHANGELOG.md
/workflows       # Interactive decision trees for tool selection
```

### Your First Feature in 30 Minutes

```
1. Make code changes
2. /test-smart            â†’ Validates affected areas only
3. /fix-auto              â†’ Auto-cleanup
4. /log-change "feat: X"  â†’ Document
5. git commit             â†’ Ship it!
```

**Human validation:** Quick code review (2-5 min)

---

## Part II: The 28 Specialized Agents

### 1. Testing & Quality Agents (7 agents)

#### ğŸ§ª **test-automator**

**Purpose:** Comprehensive test generation, TDD workflows, coverage analysis
**When to use:** Feature implementation, missing test coverage, TDD cycles
**Orchestration:** Pairs with code-reviewer for quality gates **Superpowers:**
Auto-activates `test-driven-development` skill **Example:**

```
"Generate integration tests for POST /snapshots endpoint"
â†’ Generates: API tests, service tests, edge cases
â†’ Quality: 95%+ coverage, follows project patterns
```

**Validation:** Review generated tests, run `/test-smart`

---

#### ğŸ”§ **test-repair**

**Purpose:** Autonomous test failure detection and repair **When to use:** Tests
failing after changes, flaky tests, timeout issues **Orchestration:** Called by
`/fix-auto` for complex failures **Superpowers:** Uses `systematic-debugging`
skill (4-phase framework) **Real Success:** Fixed 47 failing tests in 12 minutes
**Example:**

```
Tests failing: 47/520
â†’ test-repair agent analyzes failures
â†’ Fixes: Mock pollution (8), async timing (12), type errors (27)
â†’ Result: All tests passing in 12 minutes
```

**Validation:** Verify all tests pass, no regressions

---

#### ğŸ‘ï¸ **code-reviewer**

**Purpose:** Code quality, style checking, anti-pattern detection **When to
use:** Before every commit, PR reviews, architecture changes **Orchestration:**
Runs in parallel with comment-analyzer, type-design-analyzer **Superpowers:**
Integrates `receiving-code-review` skill **Example:**

```
Review changes in server/routes/portfolio/
â†’ Checks: CLAUDE.md conventions, anti-patterns, TypeScript strict mode
â†’ Flags: 3 issues (1 critical, 2 warnings)
â†’ Human: Fix critical, ship warnings
```

**Validation:** Address critical issues, document warnings

---

#### ğŸ’¬ **comment-analyzer**

**Purpose:** Comment accuracy verification, documentation quality analysis
**When to use:** After documentation changes, large docstrings, API docs
**Orchestration:** Runs in parallel during PR review (hybrid pipeline)
**Superpowers:** Auto-activates with docs-architect agent **Example:**

```
Analyze JSDoc in client/src/lib/waterfall.ts
â†’ Checks: Comments match implementation, params documented, return types accurate
â†’ Found: 2 stale comments (function signature changed)
```

**Validation:** Fix misleading comments, update docs

---

#### ğŸ¯ **type-design-analyzer**

**Purpose:** Type design quality assessment, encapsulation ratings **When to
use:** Introducing new types, refactoring type system **Orchestration:** Runs in
parallel with code-reviewer **Superpowers:** Provides quantitative ratings (0-10
scale) **Example:**

```
Analyze new PortfolioSnapshot type
â†’ Encapsulation: 8/10 (good private fields)
â†’ Invariant Expression: 6/10 (missing status validation)
â†’ Recommendation: Add status enum validation in constructor
```

**Validation:** Fix types rated <7, document design decisions

---

#### ğŸ”‡ **silent-failure-hunter**

**Purpose:** Find suppressed errors, inadequate error handling, silent catch
blocks **When to use:** PR review, after adding try-catch blocks, error handling
refactor **Orchestration:** Final pass after code-reviewer (sequential)
**Superpowers:** Detects anti-pattern AP-ERROR-01 (silent failures) **Example:**

```
Scan server/services/snapshot-service.ts
â†’ Found: 3 catch blocks without logging
â†’ Found: 1 catch with empty body (silently swallows error)
â†’ Recommendation: Add structured logging, correlation IDs
```

**Validation:** Add proper error handling, test error paths

---

#### ğŸ“Š **pr-test-analyzer**

**Purpose:** PR test coverage quality and completeness review **When to use:**
PR creation/update, before merge **Orchestration:** Runs after test-automator in
sequential workflow **Superpowers:** Checks for edge case coverage, critical
path validation **Example:**

```
Analyze PR #145 test coverage
â†’ Files changed: 8
â†’ Test coverage: 87% (below 90% target)
â†’ Missing: Null checks in parseWaterfall(), concurrency tests for snapshot creation
â†’ Critical gap: No test for version conflict in optimistic locking
```

**Validation:** Add critical missing tests (version conflict test is mandatory)

---

### 2. Domain Specialist Agents (2 agents)

#### ğŸ’° **waterfall-specialist** â­ **CRITICAL FOR VC CALCULATIONS**

**Purpose:** ALL waterfall/carry calculations, hurdle rates, catch-up
provisions, LP distributions **When to use:** Any American/European waterfall
logic, carry calculations, distribution modeling **Orchestration:** NEVER
parallelize (sequential only) - financial calculations require precision
**Superpowers:** Domain expert for Press On Ventures VC fund modeling
**MANDATORY RULE:** Use for ALL waterfall work - never implement from scratch
**Example:**

```
Calculate LP distributions for $500M fund with tiered hurdles:
- 8% preferred return
- 20% carry after hurdle
- 80/20 catch-up to 20% carry
â†’ waterfall-specialist agent calculates distributions
â†’ Returns: LP allocations, GP carry, carry vesting schedule
```

**Validation:** **MANDATORY** - Spot-check 2-3 calculations manually, verify
against Excel model

---

#### ğŸ“ˆ **cohort-specialist** (future)

**Purpose:** Cohort-based analytics, vintage year analysis, portfolio
segmentation **When to use:** Portfolio cohort comparisons, vintage performance
analysis **Orchestration:** Can parallelize across cohorts (independent
calculations) **Example:**

```
Compare 2019 vs 2020 vintage performance
â†’ Parallel agents calculate metrics for each cohort
â†’ Metrics: IRR, MOIC, DPI, RVPI
â†’ Synthesis: 2019 cohort outperforming by 15% IRR
```

---

### 3. Architecture & Planning Agents (4 agents)

#### ğŸ—ï¸ **architect-review** â­

**Purpose:** Architectural decisions, system design review, pattern validation
**When to use:** Major features, design decisions, architecture changes
**Orchestration:** Sequential with code-explorer (research first, then review)
**Superpowers:** Uses `brainstorming` skill for Socratic design refinement
**Example:**

```
Review portfolio API architecture for snapshots + lots
â†’ Analyzes: Service layer separation, optimistic locking strategy, cursor pagination
â†’ Validates: Anti-pattern prevention (24 patterns), CLAUDE.md conventions
â†’ Recommends: Add idempotency middleware, compound cursors (timestamp + UUID)
```

**Validation:** Team discussion, document in DECISIONS.md via `/log-decision`

---

#### ğŸ” **code-explorer**

**Purpose:** Understand existing implementations, trace execution paths **When
to use:** Before modifying unfamiliar code, investigating features
**Orchestration:** First step before implementation agents (blocking gate)
**Superpowers:** Uses `pattern-recognition` skill for cross-file analysis
**Example:**

```
"How does authentication flow work in this app?"
â†’ Traces: Login route â†’ Auth middleware â†’ JWT verification â†’ User context
â†’ Maps: 12 files across client + server
â†’ Explains: Session management, token refresh, RBAC
```

**Validation:** Confirm understanding before making changes

---

#### ğŸ“ **dx-optimizer**

**Purpose:** Developer experience improvements, workflow optimization **When to
use:** Project setup, workflow friction, onboarding pain points
**Orchestration:** Periodic reviews (monthly), standalone tasks **Superpowers:**
Optimizes tooling, reduces cognitive load **Example:**

```
"Improve test execution speed"
â†’ Analyzes: Vitest config, test parallelization, slow tests
â†’ Implements: Test projects (server/client separation), --run flag optimization
â†’ Result: Test suite 5min â†’ 30s via /test-smart
```

**Validation:** Measure before/after metrics, developer feedback

---

#### ğŸ”„ **legacy-modernizer**

**Purpose:** Refactor legacy code, update dependencies, reduce technical debt
**When to use:** Technical debt cleanup, migration projects **Orchestration:**
Sequential with test-automator (write tests first!) **Superpowers:** Uses
`defense-in-depth` skill for safe refactoring **Example:**

```
Migrate jQuery components to React
â†’ Phase 1: Add characterization tests (preserve behavior)
â†’ Phase 2: Incremental migration (one component at a time)
â†’ Phase 3: Remove jQuery dependency
â†’ Result: Modern codebase, no regressions
```

**Validation:** Full regression test suite, visual regression tests

---

### 4. Database Agents (3 agents)

#### ğŸ—„ï¸ **db-migration** â­

**Purpose:** Schema migrations with risk assessment (Safe/Risky/Dangerous
categorization) **When to use:** BEFORE every schema change (mandatory gate)
**Orchestration:** Must run first (blocking gate before db:push)
**Superpowers:** Risk analysis, downtime estimation, rollback plan generation
**Example:**

```
Assess adding "version" column to portfolios table
â†’ Risk: SAFE (adding nullable bigint column)
â†’ Downtime: <10ms (no locking, no rewrite)
â†’ Rollback: DROP COLUMN version
â†’ Recommendation: Proceed with monitoring
```

**Risk Levels:**

- **SAFE:** Add nullable column, add index (non-blocking) â†’ Proceed
- **RISKY:** Alter column type, add NOT NULL constraint â†’ Test on staging first
- **DANGEROUS:** Drop column, change primary key â†’ Manual review required,
  maintenance window

**Validation:** Review risk assessment, test on staging, document in migration
file

---

#### ğŸ¯ **database-expert**

**Purpose:** Schema design, query optimization, index strategy **When to use:**
Performance issues, new table design, query slow (>100ms) **Orchestration:**
Parallel with architect-review for schema design **Superpowers:** Understands
Drizzle ORM patterns, PostgreSQL specifics **Example:**

```
Design indexes for cursor pagination on snapshots table
â†’ Analyzes: Query pattern (ORDER BY snapshotTime DESC, id DESC)
â†’ Recommends: Compound index (snapshotTime DESC, id DESC)
â†’ Explains: Prevents full table scan, enables index-only scan
â†’ Result: Query 45s â†’ 50ms with 10k+ records
```

**Validation:** Load test with realistic data (10k+ records)

---

#### ğŸ‘¨â€ğŸ’¼ **database-admin**

**Purpose:** Operations, high availability, disaster recovery, infrastructure
**When to use:** Infrastructure setup, backup strategies, replication
**Orchestration:** Rare (infrastructure-level), standalone tasks **Example:**

```
Design PostgreSQL replication for production
â†’ Recommends: Primary-replica setup, WAL shipping
â†’ Configures: Automated failover, backup retention (7 days)
â†’ Documents: Runbook for disaster recovery
```

**Validation:** Test failover procedures, verify backup restoration

---

### 5. Infrastructure & Incident Agents (3 agents)

#### ğŸš¨ **incident-responder**

**Purpose:** P0 incident management, SRE practices, post-mortem generation
**When to use:** Production outages, service degradation, alert storms
**Orchestration:** Human-led with agent support (parallel investigation)
**Superpowers:** Post-mortem generation, runbook creation, incident timeline
**Example:**

```
API gateway returning 504 errors, 30% of users affected
â†’ Establishes: Incident command, severity P0
â†’ Investigates: Traces, logs, metrics (parallel with human team)
â†’ Identifies: Redis connection pool exhaustion
â†’ Mitigates: Increase pool size, add circuit breaker
â†’ Post-mortem: Root cause, timeline, action items
```

**Validation:** Incident resolved, post-mortem reviewed, action items tracked

---

#### ğŸ”¥ **devops-troubleshooter**

**Purpose:** Production issues, deployment problems, infrastructure debugging
**When to use:** Build failures, deployment errors, container issues
**Orchestration:** Parallel investigation of independent issues **Superpowers:**
Uses `systematic-debugging` skill (4-phase framework) **Example:**

```
Kubernetes pod OOMKilled, investigate memory usage
â†’ Analyzes: Pod logs, memory metrics, heap dumps
â†’ Identifies: Memory leak in worker process (unclosed connections)
â†’ Fixes: Add connection cleanup, increase memory limit
â†’ Monitors: Memory usage over 24 hours
```

**Validation:** Issue resolved, monitoring confirms stability

---

#### ğŸ’£ **chaos-engineer**

**Purpose:** Resilience testing, failure injection, game day planning **When to
use:** Pre-production validation, quarterly game days **Orchestration:**
Controlled experiments only (never in production) **Superpowers:** Designs game
day scenarios, failure injection strategies **Example:**

```
Test Redis down/restart recovery
â†’ Experiment: Kill Redis during high load
â†’ Observes: Queue jobs pause, graceful degradation
â†’ Validates: Jobs resume after Redis restart, no data loss
â†’ Documents: Recovery time 15s, zero errors
```

**Validation:** System handles failure gracefully, recovery procedures
documented

---

### 6. Documentation Agents (2 agents)

#### ğŸ“š **docs-architect** â­

**Purpose:** Comprehensive technical documentation, API docs, architecture
guides **When to use:** Major features complete, API documentation needed
**Orchestration:** **BEST FOR PARALLEL** (8-10x speedup) - independent modules
**Superpowers:** 95-99% quality (Promptfoo validated) **Proven Success:** Week
46 documentation sprint **Real Example:**

```
Generate 5 NotebookLM modules (reserves, pacing, cohorts, waterfall, XIRR)
â†’ 8 docs-architect agents in parallel
â†’ Output: 2,400+ lines of documentation
â†’ Time: 45 minutes (vs 5.5 hours sequential)
â†’ Quality: 95-99% (Promptfoo evaluation)
â†’ Cost: $0.70 in API tokens
â†’ Human validation: Spot-check 2-3 modules (10 min)
```

**Validation:** Spot-check 2-3 modules, verify code examples compile

---

#### ğŸ› **debug-expert**

**Purpose:** Error analysis, root cause investigation, complex debugging **When
to use:** Complex production bugs, mysterious failures **Orchestration:**
Sequential with `systematic-debugging` skill **Superpowers:** Four-phase
framework (root cause â†’ pattern â†’ hypothesis â†’ fix) **Example:**

```
XIRR calculation returning NaN for certain portfolios
â†’ Phase 1: Root cause investigation (trace calculation)
â†’ Phase 2: Pattern analysis (all failing cases have zero cash flows)
â†’ Phase 3: Hypothesis testing (division by zero check)
â†’ Phase 4: Implementation (add validation for zero cash flows)
```

**Validation:** Verify fix resolves issue, add regression test

---

### 7. General Purpose Agents (4 agents)

#### ğŸ¯ **general-purpose**

**Purpose:** Complex research, multi-step tasks, open-ended investigation **When
to use:** Exploratory work, understanding codebase structure **Orchestration:**
Can spawn sub-tasks, recursive investigation **Superpowers:** All tools
available (Read, Edit, Bash, etc.) **Example:**

```
"Explore codebase structure and explain the waterfall module"
â†’ Reads: client/src/lib/waterfall.ts, tests, components using it
â†’ Maps: Data flow from UI â†’ validation â†’ calculation â†’ display
â†’ Explains: American vs European waterfall types, helpers, test coverage
```

**Validation:** Review findings, ask clarifying questions

---

#### ğŸ§  **context-orchestrator** â­

**Purpose:** Multi-agent workflow coordination, 8-10x speedup orchestrator
**When to use:** Complex tasks requiring multiple specialized agents
**Orchestration:** **META-ORCHESTRATOR** (coordinates other agents)
**Superpowers:** Manages agent dependencies, token budgets, parallel execution
**Example:**

```
Coordinate PR review across 6 agents:
â†’ Parallel execution: code-reviewer, comment-analyzer, type-design-analyzer,
   pr-test-analyzer, silent-failure-hunter, code-simplifier
â†’ Manages: Token budgets, timeouts, dependency resolution
â†’ Synthesizes: Unified review report with prioritized issues
â†’ Time: 3-5 minutes (vs 15-20 minutes sequential)
```

**Validation:** Monitor orchestration dashboard, review synthesized report

---

#### ğŸ“– **knowledge-synthesizer**

**Purpose:** Extract patterns from agent interactions, build collective
intelligence **When to use:** Daily/weekly (background), after major sprints
**Orchestration:** Runs periodically (automated) **Superpowers:** Pattern
learning, cross-session memory, insight generation **Example:**

```
Synthesize patterns from 50+ agent runs this week
â†’ Pattern 1: waterfall-specialist most effective with explicit test cases
â†’ Pattern 2: test-repair succeeds 90% on mock pollution issues
â†’ Pattern 3: db-migration flagged 3 RISKY migrations (all valid)
â†’ Recommendation: Update anti-pattern rules based on false positive rate
```

**Validation:** Review insights, update cheatsheets with new patterns

---

#### ğŸ”§ **code-simplifier**

**Purpose:** Simplify complex code while preserving functionality **When to
use:** After implementing features, before PR, code review **Orchestration:**
Final pass after code-reviewer (sequential) **Superpowers:** Auto-activates
after coding tasks **Example:**

```
Simplify nested conditionals in waterfall calculation
â†’ Before: 5 levels of nested if/else (cyclomatic complexity 18)
â†’ After: Guard clauses + early returns (complexity 6)
â†’ Tests: All pass, logic unchanged
â†’ Readability: Improved from 40% â†’ 85% (CodeClimate)
```

**Validation:** Tests still pass, logic unchanged, readability improved

---

### 8. Performance & Security Agents (3 agents)

#### âš¡ **perf-guard**

**Purpose:** Performance regression detection, bundle analysis, baseline
comparison **When to use:** Dependency changes, before production deploy, weekly
health check **Orchestration:** Runs in CI/CD pipeline (automated)
**Superpowers:** Baseline comparison, trend analysis, cost-benefit analysis
**Example:**

```
Detect bundle size regression after dependency update
â†’ Baseline: 245 KB (gzip)
â†’ Current: 318 KB (gzip) â†’ 30% increase âš ï¸
â†’ Culprit: New chart library (75 KB)
â†’ Recommendation: Use lightweight alternative or lazy load
```

**Validation:** Investigate regressions, optimize or accept trade-off

---

#### ğŸ” **security-comprehensive** (marketplace agent)

**Purpose:** Security scanning, vulnerability detection, OWASP top 10 **When to
use:** Weekly security scans, before production deploy **Orchestration:**
Parallel with `npm audit` **Example:**

```
Scan for SQL injection vulnerabilities
â†’ Found: Manual string concatenation in cursor query (AP-CURSOR-06)
â†’ Risk: HIGH (SQL injection possible)
â†’ Fix: Use parameterized queries via Drizzle ORM
```

**Validation:** Fix critical vulnerabilities immediately, schedule medium/low

---

#### ğŸ¨ **accessibility-compliance** (marketplace agent)

**Purpose:** WCAG compliance testing, a11y validation **When to use:** UI
component changes, before launch **Orchestration:** Parallel with UI tests
**Example:**

```
Check WCAG 2.1 AA compliance for dashboard
â†’ Found: Missing alt text on 3 chart images
â†’ Found: Color contrast ratio 3.2:1 (below 4.5:1 minimum)
â†’ Found: No keyboard navigation for modal dialog
```

**Validation:** Fix accessibility issues, re-run validation

---

## Part III: Multi-Agent Orchestration Patterns

### The Three Orchestration Modes

#### MODE 1: PARALLEL INDEPENDENT (87-91% time savings)

**Best for:** Parallelizable tasks with zero dependencies

**Real Example: Phase 1 Documentation (Week 46)**

```typescript
// Launch 8 docs-architect agents in parallel
const results = await Promise.all([
  Task({ agent: 'docs-architect', module: 'reserves' }),
  Task({ agent: 'docs-architect', module: 'pacing' }),
  Task({ agent: 'docs-architect', module: 'cohorts' }),
  Task({ agent: 'docs-architect', module: 'waterfall' }),
  Task({ agent: 'docs-architect', module: 'xirr' }),
  Task({ agent: 'docs-architect', module: 'fees' }),
  Task({ agent: 'docs-architect', module: 'analytics' }),
  Task({ agent: 'docs-architect', module: 'forecasting' }),
]);

// Results:
// - Output: 2,400+ lines of documentation
// - Time: 45 minutes (vs 5.5 hours sequential)
// - Speedup: 8-10x
// - Quality: 95-99% (Promptfoo validated)
// - Cost: $0.70 in API tokens
// - Human time: 10 min spot-check (2-3 modules)
```

**When to use:**

- Documentation generation (independent modules)
- Batch operations (linting 100 files)
- Independent code reviews (multiple PRs)
- Test suite execution (parallel test runners)

**Validation Pattern:**

```
1. All agents complete
2. Spot-check 2-3 outputs (random sample)
3. IF issues found: Fix those specific outputs
4. ELSE: Trust the rest (95-99% quality proven)
5. Ship it!
```

**Caution:** Not suitable for tasks with shared state or dependencies

---

#### MODE 2: SEQUENTIAL WITH QUALITY GATES (30-50% savings)

**Best for:** Multi-step workflows with dependencies and human validation
checkpoints

**Real Example: Feature Implementation with Gates**

```typescript
// Phase 1: Architecture Review (blocking gate)
const architecture = await Task({
  agent: 'architect-review',
  task: 'Review portfolio snapshot API design',
});

// ğŸ›‘ GATE 1: Human reviews architecture (5 min)
console.log('CHECKPOINT: Review architecture plan');
if (!humanApproves(architecture)) {
  throw new Error('Design needs revision');
}

// Phase 2: Implementation (blocking on approval)
const implementation = await Task({
  agent: 'general-purpose',
  task: `Implement based on: ${architecture.plan}`,
});

// ğŸ›‘ GATE 2: Code review (2 min)
await Task({
  agent: 'code-reviewer',
  files: implementation.files,
});

// Phase 3: Test Generation (blocking on implementation)
const tests = await Task({
  agent: 'test-automator',
  files: implementation.files,
});

// ğŸ›‘ GATE 3: Run tests and verify (2 min)
await bash('/test-smart');
console.log('All tests passing? Proceed to commit');

// Phase 4: Documentation (blocking on tests)
await bash('/log-change "feat: Add portfolio snapshot API"');

// Ship it!
```

**When to use:**

- High-risk changes (schema migrations, waterfall logic)
- First-time workflows (new patterns)
- Complex features with multiple dependencies
- Learning new domain area

**Validation Pattern:**

```
1. Human checkpoint after each phase (2-5 min each)
2. Block if critical issues found
3. Continue if "good enough for MVP"
4. Document decisions at each gate
```

**Time Breakdown Example:**

```
Architecture review: 10 min (5 min agent + 5 min human)
Implementation: 30 min (25 min agent + 5 min human review)
Testing: 15 min (12 min agent + 3 min run tests)
Documentation: 2 min
Total: ~57 min (vs 2 hours manual)
Savings: 52%
```

---

#### MODE 3: HYBRID PIPELINE (50-75% savings)

**Best for:** Complex workflows mixing independent and dependent tasks

**Real Example: PR Review Pipeline**

```typescript
// STAGE 1: Parallel Research (independent analyses)
const [codeIssues, commentIssues, typeIssues, testIssues, silentErrors] =
  await Promise.all([
    Task({ agent: 'code-reviewer', pr: 123 }),
    Task({ agent: 'comment-analyzer', pr: 123 }),
    Task({ agent: 'type-design-analyzer', pr: 123 }),
    Task({ agent: 'pr-test-analyzer', pr: 123 }),
    Task({ agent: 'silent-failure-hunter', pr: 123 }),
  ]);

// ğŸ›‘ GATE: Human reviews critical issues (5 min)
const criticalIssues = [...codeIssues, ...typeIssues, ...silentErrors].filter(
  (i) => i.severity === 'CRITICAL'
);

console.log(`Found ${criticalIssues.length} critical issues`);

if (criticalIssues.length > 0) {
  // STAGE 2: Sequential Fixes (dependent on issue identification)
  await Task({
    agent: 'general-purpose',
    task: `Fix critical issues: ${JSON.stringify(criticalIssues)}`,
  });

  // ğŸ›‘ GATE: Re-run affected reviews (blocking on fixes)
  await Task({
    agent: 'code-reviewer',
    files: criticalIssues.map((i) => i.file),
  });
}

// STAGE 3: Parallel Cleanup (independent again)
await Promise.all([
  Task({ agent: 'code-simplifier', files: 'all' }),
  bash('/fix-auto'),
  bash('/test-smart'),
]);

// ğŸ›‘ GATE: Final smoke test (2 min)
console.log('Manual smoke test: Does it work?');

// Ship it!
```

**When to use:**

- PR reviews (parallel analysis, sequential fixes, parallel cleanup)
- Deployment preparation (parallel validation, sequential build, parallel tests)
- Refactoring projects (parallel understanding, sequential changes, parallel
  validation)

**Time Breakdown Example:**

```
Parallel research: 3 min (5 agents Ã— 3 min each, but parallel)
Human review: 5 min
Sequential fixes: 10 min (if needed)
Parallel cleanup: 2 min
Final smoke test: 2 min
Total: ~22 min (vs 1.5 hours manual)
Savings: 75%
```

---

### Orchestration Decision Tree

```
â”Œâ”€ Is the task brand new or high risk?
â”‚
â”œâ”€ YES â†’ Use SEQUENTIAL WITH GATES
â”‚  â”‚     (waterfall calculations, schema changes, novel features)
â”‚  â”‚     Human checkpoints every 2-3 steps
â”‚  â”‚     Time savings: 30-50%
â”‚  â”‚
â”‚  â””â”€ Example: New waterfall type implementation
â”‚        â†’ db-migration (risk check) â†’ (gate) â†’ implement â†’ (gate) â†’ test
â”‚
â””â”€ NO â†’ Are sub-tasks independent?
   â”‚
   â”œâ”€ YES â†’ Use PARALLEL INDEPENDENT
   â”‚  â”‚     (documentation, batch operations, independent modules)
   â”‚  â”‚     Maximum speed, spot-check validation
   â”‚  â”‚     Time savings: 87-91%
   â”‚  â”‚
   â”‚  â””â”€ Example: Generate 8 documentation modules
   â”‚        â†’ 8 docs-architect agents in parallel â†’ spot-check 2-3
   â”‚
   â””â”€ NO â†’ Use HYBRID PIPELINE
          (PR reviews, deployment prep, refactoring)
          Parallel research â†’ Sequential fixes â†’ Parallel cleanup
          Time savings: 50-75%

          Example: PR review
          â†’ 5 review agents parallel â†’ human gate â†’ fixes â†’ cleanup parallel
```

**Quick Decision Guide:**

| Task Type                           | Mode        | Time Savings | Example                    |
| ----------------------------------- | ----------- | ------------ | -------------------------- |
| Documentation (independent modules) | Parallel    | 87-91%       | 8 modules in 45 min        |
| Batch operations (linting files)    | Parallel    | 87-91%       | Lint 100 files             |
| New feature (dependencies)          | Sequential  | 30-50%       | API endpoint with tests    |
| Schema migration                    | Sequential  | 30-50%       | Add version column         |
| PR review                           | Hybrid      | 50-75%       | 5 agents â†’ fixes â†’ cleanup |
| Deployment prep                     | Hybrid      | 50-75%       | Validate â†’ build â†’ test    |
| Bug fix (simple)                    | `/fix-auto` | 60-90%       | Lint + type errors         |
| Bug fix (complex)                   | Sequential  | 30-50%       | Systematic debugging       |

---

### Real Performance Data from This Project

#### Success Story 1: Week 46 Documentation Sprint

```
Task: Generate 5 NotebookLM modules for Phase 1 documentation
Agents: 8 Ã— docs-architect (parallel independent)
Time: 45 minutes
Output: 2,400+ lines
Quality: 95-99% (Promptfoo validated)
Cost: $0.70 in API tokens
Sequential estimate: 5.5 hours
Speedup: 7.3x
Time saved: 4 hours 45 minutes
Human validation: 10 minutes (spot-check 2-3 modules)
```

#### Success Story 2: Test Repair Automation

```
Task: Fix 47 failing tests after dependency update
Agent: test-repair (sequential with systematic debugging)
Time: 12 minutes
Fixes: Mock pollution (8), async timing (12), type errors (27)
Sequential estimate: 2-3 hours manual debugging
Speedup: 10-15x
Time saved: 2+ hours
Human validation: 5 minutes (verify tests pass, spot-check fixes)
```

#### Success Story 3: PR Review Pipeline

```
Task: Review PR with 15 changed files
Agents: 6 agents (hybrid pipeline)
  - Parallel: code-reviewer, comment-analyzer, type-design-analyzer,
     pr-test-analyzer, silent-failure-hunter (3 min)
  - Sequential: Fix critical issues (10 min)
  - Parallel: code-simplifier, /fix-auto, /test-smart (2 min)
Total time: 15 minutes
Sequential estimate: 45-60 minutes manual review
Speedup: 3-4x
Time saved: 30-45 minutes
Human validation: 5 minutes (review critical issues, approve fixes)
```

---

## Part IV: Coding Pairs Workflows

### The Three Pairing Patterns

#### PATTERN A: Driver-Navigator (AI Drives, Human Navigates)

**Best for:** Implementing well-defined features with human guidance

**Workflow:**

```
Human (Navigator): "Add snapshot creation endpoint to portfolio API"

AI (Driver): "I'll implement in 10-20 line chunks with checkpoints:

ğŸ“ Chunk 1: Define Zod schema for CreateSnapshotRequest
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
import { z } from 'zod';

export const CreateSnapshotRequestSchema = z.object({
  name: z.string().min(1).max(255),
  fundId: z.number().int().positive(),
  idempotencyKey: z.string().uuid(),
});

âœ“ Code review with code-reviewer agent
âœ“ /test-smart (if applicable)
ğŸ›‘ CHECKPOINT: Does this schema look correct?"
```
