# Promptfoo Configuration for Fee Documentation Validation
# Phase 1B: Management & Performance Fees Documentation
# Anthropic Cookbook LLM-as-Judge Pattern

description: 'Phase 1B Fee Documentation Validation - Management & Performance Fees'

prompts:
  - file://prompts.py:fee_prompt

# Claude models to use for evaluation
# Note: API key has access to Opus, not Sonnet 3.5
providers:
  - id: anthropic:messages:claude-3-opus-20240229
    label: 'Claude 3 Opus'
    config:
      max_tokens: 4096 # Opus max output tokens
      temperature: 0

# Default assertions that apply to all tests
defaultTest:
  assert:
    # Ensure no AI self-reference
    - type: not-contains-any
      value:
        - 'I am an AI'
        - 'As an AI language model'
        - "I'm an AI assistant"
        - 'I am a chatbot'

# Test cases for fee documentation validation
tests:
  # Test 1: fees.md validation
  - description: 'Validate fees.md against Phase 1 rubric'
    vars:
      doc_content: file://../../docs/notebooklm-sources/fees.md
      truth_cases: file://../../docs/fees.truth-cases.json
      schema: file://../../docs/schemas/fee-truth-case.schema.json
      doc_type: 'primary_documentation'
    assert:
      # LLM-as-Judge evaluator (Anthropic Cookbook pattern)
      - type: python
        value: file://custom_evals/doc_llm_eval.py
        threshold: 0.75

      # Content coverage checks (optional - LLM evaluator already covers these)
      - type: icontains-all
        value:
          - 'management fee'
          - 'performance fee'
          - 'fee basis'
          - 'fee timing'
          - 'hurdle'

      # Mathematical content checks
      - type: icontains-any
        value:
          - 'formula'
          - 'calculation'
          - 'computeManagementFee'
          - 'computePerformanceFee'

      # Integration checks
      - type: icontains-all
        value:
          - 'client/src/lib/fee-calculations.ts'
          - 'shared/schemas/fee-policy.ts'
          - 'ManagementFee'
          - 'PerformanceFee'

  # Test 2: ADR-006 validation
  - description: 'Validate ADR-006 architectural decisions'
    vars:
      doc_content: file://../../docs/adr/ADR-006-fee-calculation-standards.md
      truth_cases: file://../../docs/fees.truth-cases.json
      schema: file://../../docs/schemas/fee-truth-case.schema.json
      doc_type: 'architecture_decision_record'
    assert:
      # LLM-as-Judge evaluator (Anthropic Cookbook pattern)
      - type: python
        value: file://custom_evals/doc_llm_eval.py
        threshold: 0.75

      # ADR structure checks
      - type: icontains-all
        value:
          - 'Status'
          - 'Accepted'
          - 'Context'
          - 'Decision'
          - 'Consequences'

      # Decision content checks
      - type: icontains-all
        value:
          - 'Management Fee Structure'
          - 'Performance Fee Models'
          - 'Fee Timing'
          - 'Hurdle Implementation'

      # Integration validation for ADRs
      - type: javascript
        value: |
          const matches = output.match(/\w+\.(ts|tsx|js|mjs):\d+/g) || [];
          return {
            pass: matches.length >= 5,
            score: Math.min(1, matches.length / 10),
            reason: matches.length >= 5
              ? `Found ${matches.length} file:line references`
              : `Expected at least 5 file:line references, found ${matches.length}`
          };

# Note: Custom Python LLM evaluator is now in the defaultTest assert section above

# Output configuration
outputPath: ./results/fees-validation-results.json

# Enable progress bar during evaluation
evaluateOptions:
  showProgressBar: true
  maxConcurrency: 1 # Sequential execution to avoid rate limits
