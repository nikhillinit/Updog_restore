{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extended Thinking for All Claude Models\n",
    "\n",
    "## Table of contents\n",
    "- [Setup](#setup)\n",
    "- [Model Capabilities](#model-capabilities)\n",
    "- [Basic Examples](#basic-examples)\n",
    "- [Streaming with Extended Thinking](#streaming-with-extended-thinking)\n",
    "- [Token Management](#token-management)\n",
    "- [Advanced Patterns](#advanced-patterns)\n",
    "- [Integration with AI Agents](#integration-with-ai-agents)\n",
    "- [Error Handling](#error-handling)\n",
    "\n",
    "This notebook demonstrates how to use extended thinking across all Claude models, with examples for integrating into your AI agent system.\n",
    "\n",
    "Extended thinking gives Claude enhanced reasoning capabilities for complex tasks, while providing transparency into its step-by-step thought process. When enabled, Claude creates `thinking` content blocks where it outputs internal reasoning before crafting a final response.\n",
    "\n",
    "For more information, see the [official documentation](https://docs.claude.com/en/docs/build-with-claude/extended-thinking)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Install required packages and configure the environment for all Claude models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install anthropic typing_extensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import anthropic\nimport os\nimport json\nfrom typing import Dict, Any, Optional, List, Union\nfrom enum import Enum\n\n# Set your API key\n# os.environ[\"ANTHROPIC_API_KEY\"] = \"your-api-key-here\"\n\n# Initialize the client\nclient = anthropic.Anthropic()\n\nclass ClaudeModel(Enum):\n    \"\"\"Available Claude models with extended thinking support.\"\"\"\n    OPUS_4_5 = \"claude-opus-4-5-20251101\"  # Latest with extended thinking\n    SONNET_4_5 = \"claude-sonnet-4-5-20250929\"\n    OPUS_3 = \"claude-3-opus-20240229\"\n    HAIKU_3 = \"claude-3-haiku-20240307\"\n    \n    @property\n    def supports_thinking(self) -> bool:\n        \"\"\"Check if model supports extended thinking.\"\"\"\n        return self in [ClaudeModel.OPUS_4_5, ClaudeModel.SONNET_4_5]\n    \n    @property\n    def default_thinking_budget(self) -> int:\n        \"\"\"Get default thinking budget for model.\"\"\"\n        if self == ClaudeModel.OPUS_4_5:\n            return 10000\n        elif self == ClaudeModel.SONNET_4_5:\n            return 8000\n        else:\n            return 0\n\n# Model capabilities matrix\nMODEL_CAPABILITIES = {\n    ClaudeModel.OPUS_4_5: {\n        \"extended_thinking\": True,\n        \"max_thinking_budget\": 65536,\n        \"recommended_budget\": 10000,\n        \"context_window\": 200000\n    },\n    ClaudeModel.SONNET_4_5: {\n        \"extended_thinking\": True,\n        \"max_thinking_budget\": 65536,\n        \"recommended_budget\": 8000,\n        \"context_window\": 200000\n    },\n    ClaudeModel.OPUS_3: {\n        \"extended_thinking\": False,\n        \"max_thinking_budget\": 0,\n        \"recommended_budget\": 0,\n        \"context_window\": 200000\n    },\n    ClaudeModel.HAIKU_3: {\n        \"extended_thinking\": False,\n        \"max_thinking_budget\": 0,\n        \"recommended_budget\": 0,\n        \"context_window\": 200000\n    }\n}\n\nprint(\"Model Capabilities:\")\nfor model in ClaudeModel:\n    caps = MODEL_CAPABILITIES[model]\n    print(f\"\\n{model.name}:\")\n    print(f\"  - Extended Thinking: {caps['extended_thinking']}\")\n    print(f\"  - Max Budget: {caps['max_thinking_budget']:,} tokens\")\n    print(f\"  - Context Window: {caps['context_window']:,} tokens\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Capabilities\n",
    "\n",
    "Different Claude models have varying support for extended thinking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExtendedThinkingManager:\n",
    "    \"\"\"Manager for extended thinking across all Claude models.\"\"\"\n",
    "    \n",
    "    def __init__(self, client: anthropic.Anthropic):\n",
    "        self.client = client\n",
    "        \n",
    "    def create_message(\n",
    "        self,\n",
    "        model: ClaudeModel,\n",
    "        messages: List[Dict[str, str]],\n",
    "        thinking_budget: Optional[int] = None,\n",
    "        max_tokens: int = 4000,\n",
    "        **kwargs\n",
    "    ) -> Any:\n",
    "        \"\"\"Create a message with model-appropriate thinking configuration.\"\"\"\n",
    "        \n",
    "        # Check model capabilities\n",
    "        caps = MODEL_CAPABILITIES[model]\n",
    "        \n",
    "        # Prepare base parameters\n",
    "        params = {\n",
    "            \"model\": model.value,\n",
    "            \"messages\": messages,\n",
    "            \"max_tokens\": max_tokens\n",
    "        }\n",
    "        \n",
    "        # Add thinking if supported\n",
    "        if caps[\"extended_thinking\"]:\n",
    "            budget = thinking_budget or caps[\"recommended_budget\"]\n",
    "            # Ensure budget is within limits\n",
    "            budget = min(budget, caps[\"max_thinking_budget\"])\n",
    "            budget = max(budget, 1024)  # Minimum budget\n",
    "            \n",
    "            params[\"thinking\"] = {\n",
    "                \"type\": \"enabled\",\n",
    "                \"budget_tokens\": budget\n",
    "            }\n",
    "            print(f\"‚úì Extended thinking enabled with {budget:,} token budget\")\n",
    "        else:\n",
    "            print(f\"‚Ñπ {model.name} doesn't support extended thinking, using standard mode\")\n",
    "        \n",
    "        # Add any additional parameters\n",
    "        params.update(kwargs)\n",
    "        \n",
    "        return self.client.messages.create(**params)\n",
    "    \n",
    "    def process_response(self, response) -> Dict[str, Any]:\n",
    "        \"\"\"Process response and extract thinking + final answer.\"\"\"\n",
    "        result = {\n",
    "            \"thinking_blocks\": [],\n",
    "            \"redacted_blocks\": [],\n",
    "            \"final_answer\": \"\",\n",
    "            \"has_thinking\": False\n",
    "        }\n",
    "        \n",
    "        for block in response.content:\n",
    "            if block.type == \"thinking\":\n",
    "                result[\"thinking_blocks\"].append({\n",
    "                    \"content\": block.thinking,\n",
    "                    \"signature\": getattr(block, 'signature', None)\n",
    "                })\n",
    "                result[\"has_thinking\"] = True\n",
    "            elif block.type == \"redacted_thinking\":\n",
    "                result[\"redacted_blocks\"].append({\n",
    "                    \"data\": block.data if hasattr(block, 'data') else None\n",
    "                })\n",
    "                result[\"has_thinking\"] = True\n",
    "            elif block.type == \"text\":\n",
    "                result[\"final_answer\"] += block.text\n",
    "        \n",
    "        return result\n",
    "\n",
    "# Initialize manager\n",
    "thinking_manager = ExtendedThinkingManager(client)\n",
    "\n",
    "print(\"Extended Thinking Manager initialized!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Examples\n",
    "\n",
    "Examples showing extended thinking across different models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model_thinking(model: ClaudeModel, prompt: str):\n",
    "    \"\"\"Test extended thinking with a specific model.\"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Testing {model.name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    try:\n",
    "        response = thinking_manager.create_message(\n",
    "            model=model,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            thinking_budget=2000  # Will be ignored for non-supporting models\n",
    "        )\n",
    "        \n",
    "        result = thinking_manager.process_response(response)\n",
    "        \n",
    "        if result[\"has_thinking\"]:\n",
    "            print(f\"\\nüß† THINKING DETECTED:\")\n",
    "            for i, block in enumerate(result[\"thinking_blocks\"][:1]):  # Show first block\n",
    "                preview = block[\"content\"][:300] + \"...\" if len(block[\"content\"]) > 300 else block[\"content\"]\n",
    "                print(f\"  Block {i+1}: {preview}\")\n",
    "        \n",
    "        print(f\"\\n‚úÖ FINAL ANSWER:\")\n",
    "        preview = result[\"final_answer\"][:500] + \"...\" if len(result[\"final_answer\"]) > 500 else result[\"final_answer\"]\n",
    "        print(preview)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error: {e}\")\n",
    "\n",
    "# Test prompt\n",
    "test_prompt = \"\"\"Solve this step by step: \n",
    "If a train leaves Chicago at 2 PM traveling east at 60 mph, \n",
    "and another train leaves New York at 3 PM traveling west at 80 mph, \n",
    "and the distance between the cities is 790 miles, \n",
    "when and where do they meet?\"\"\"\n",
    "\n",
    "# Test with different models (comment out to avoid API calls)\n",
    "# test_model_thinking(ClaudeModel.SONNET_4_5, test_prompt)\n",
    "# test_model_thinking(ClaudeModel.SONNET_3_5, test_prompt)\n",
    "# test_model_thinking(ClaudeModel.OPUS_3, test_prompt)\n",
    "\n",
    "print(\"\\nTo run tests, uncomment the test_model_thinking() calls above\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Streaming with Extended Thinking\n",
    "\n",
    "Handle streaming responses with thinking blocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StreamingThinkingHandler:\n",
    "    \"\"\"Handler for streaming responses with thinking.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.current_block_type = None\n",
    "        self.current_content = \"\"\n",
    "        self.thinking_content = []\n",
    "        self.final_answer = \"\"\n",
    "        \n",
    "    def handle_event(self, event) -> Optional[str]:\n",
    "        \"\"\"Process streaming event and return display text if needed.\"\"\"\n",
    "        \n",
    "        if event.type == \"content_block_start\":\n",
    "            self.current_block_type = event.content_block.type\n",
    "            self.current_content = \"\"\n",
    "            \n",
    "            if self.current_block_type == \"thinking\":\n",
    "                return \"\\nüß† [Thinking...]\"\n",
    "            elif self.current_block_type == \"text\":\n",
    "                return \"\\n‚úÖ [Generating answer...]\\n\"\n",
    "                \n",
    "        elif event.type == \"content_block_delta\":\n",
    "            if event.delta.type == \"thinking_delta\":\n",
    "                self.current_content += event.delta.thinking\n",
    "                # Return dots to show progress without cluttering output\n",
    "                return \".\"\n",
    "            elif event.delta.type == \"text_delta\":\n",
    "                self.current_content += event.delta.text\n",
    "                self.final_answer += event.delta.text\n",
    "                return event.delta.text  # Stream the actual text\n",
    "                \n",
    "        elif event.type == \"content_block_stop\":\n",
    "            if self.current_block_type == \"thinking\":\n",
    "                self.thinking_content.append(self.current_content)\n",
    "                return f\" [{len(self.current_content)} chars]\\n\"\n",
    "            self.current_block_type = None\n",
    "            \n",
    "        elif event.type == \"message_stop\":\n",
    "            return \"\\n\\n[Complete]\"\n",
    "            \n",
    "        return None\n",
    "\n",
    "def stream_with_thinking(model: ClaudeModel, prompt: str):\n",
    "    \"\"\"Stream a response with thinking blocks.\"\"\"\n",
    "    \n",
    "    if not MODEL_CAPABILITIES[model][\"extended_thinking\"]:\n",
    "        print(f\"‚ö†Ô∏è {model.name} doesn't support extended thinking\")\n",
    "        return\n",
    "    \n",
    "    handler = StreamingThinkingHandler()\n",
    "    \n",
    "    params = {\n",
    "        \"model\": model.value,\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
    "        \"max_tokens\": 4000\n",
    "    }\n",
    "    \n",
    "    if MODEL_CAPABILITIES[model][\"extended_thinking\"]:\n",
    "        params[\"thinking\"] = {\n",
    "            \"type\": \"enabled\",\n",
    "            \"budget_tokens\": MODEL_CAPABILITIES[model][\"recommended_budget\"]\n",
    "        }\n",
    "    \n",
    "    with client.messages.stream(**params) as stream:\n",
    "        for event in stream:\n",
    "            output = handler.handle_event(event)\n",
    "            if output:\n",
    "                print(output, end=\"\", flush=True)\n",
    "    \n",
    "    return handler\n",
    "\n",
    "# Example usage (uncomment to run)\n",
    "# handler = stream_with_thinking(\n",
    "#     ClaudeModel.SONNET_4_5,\n",
    "#     \"What are the key considerations when designing a distributed cache?\"\n",
    "# )\n",
    "\n",
    "print(\"Streaming handler configured. Uncomment example to test.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Token Management\n",
    "\n",
    "Optimize token usage with extended thinking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ThinkingBudgetOptimizer:\n",
    "    \"\"\"Optimize thinking budget based on task complexity.\"\"\"\n",
    "    \n",
    "    TASK_COMPLEXITY_BUDGETS = {\n",
    "        \"simple\": 1024,      # Basic questions, simple logic\n",
    "        \"moderate\": 2048,    # Multi-step problems, moderate analysis\n",
    "        \"complex\": 4096,     # Complex reasoning, detailed analysis\n",
    "        \"deep\": 8192,        # Deep analysis, research tasks\n",
    "        \"extensive\": 16384,  # Extensive reasoning, complex proofs\n",
    "        \"maximum\": 32768     # Maximum depth analysis\n",
    "    }\n",
    "    \n",
    "    @staticmethod\n",
    "    def estimate_complexity(prompt: str) -> str:\n",
    "        \"\"\"Estimate task complexity from prompt.\"\"\"\n",
    "        \n",
    "        # Keywords indicating complexity\n",
    "        simple_keywords = [\"what is\", \"define\", \"explain briefly\", \"summarize\"]\n",
    "        moderate_keywords = [\"how does\", \"compare\", \"analyze\", \"describe\"]\n",
    "        complex_keywords = [\"step by step\", \"prove\", \"derive\", \"implement\"]\n",
    "        deep_keywords = [\"research\", \"investigate\", \"comprehensive\", \"detailed analysis\"]\n",
    "        extensive_keywords = [\"exhaustive\", \"complete proof\", \"all possibilities\", \"thoroughly\"]\n",
    "        \n",
    "        prompt_lower = prompt.lower()\n",
    "        \n",
    "        # Check keywords in order of complexity\n",
    "        if any(kw in prompt_lower for kw in extensive_keywords):\n",
    "            return \"extensive\"\n",
    "        elif any(kw in prompt_lower for kw in deep_keywords):\n",
    "            return \"deep\"\n",
    "        elif any(kw in prompt_lower for kw in complex_keywords):\n",
    "            return \"complex\"\n",
    "        elif any(kw in prompt_lower for kw in moderate_keywords):\n",
    "            return \"moderate\"\n",
    "        elif any(kw in prompt_lower for kw in simple_keywords):\n",
    "            return \"simple\"\n",
    "        else:\n",
    "            # Default based on prompt length\n",
    "            if len(prompt) > 500:\n",
    "                return \"complex\"\n",
    "            elif len(prompt) > 200:\n",
    "                return \"moderate\"\n",
    "            else:\n",
    "                return \"simple\"\n",
    "    \n",
    "    @classmethod\n",
    "    def get_optimal_budget(\n",
    "        cls, \n",
    "        prompt: str, \n",
    "        model: ClaudeModel,\n",
    "        override_complexity: Optional[str] = None\n",
    "    ) -> int:\n",
    "        \"\"\"Get optimal thinking budget for prompt and model.\"\"\"\n",
    "        \n",
    "        # Check if model supports thinking\n",
    "        if not MODEL_CAPABILITIES[model][\"extended_thinking\"]:\n",
    "            return 0\n",
    "        \n",
    "        # Determine complexity\n",
    "        complexity = override_complexity or cls.estimate_complexity(prompt)\n",
    "        \n",
    "        # Get base budget\n",
    "        budget = cls.TASK_COMPLEXITY_BUDGETS.get(complexity, 2048)\n",
    "        \n",
    "        # Ensure within model limits\n",
    "        max_budget = MODEL_CAPABILITIES[model][\"max_thinking_budget\"]\n",
    "        budget = min(budget, max_budget)\n",
    "        \n",
    "        # Ensure minimum budget\n",
    "        budget = max(budget, 1024)\n",
    "        \n",
    "        return budget\n",
    "\n",
    "# Test complexity estimation\n",
    "test_prompts = [\n",
    "    \"What is Python?\",\n",
    "    \"Explain how neural networks work\",\n",
    "    \"Implement a red-black tree with full balancing\",\n",
    "    \"Provide a comprehensive analysis of quantum computing applications\",\n",
    "    \"Exhaustively prove the correctness of this algorithm\"\n",
    "]\n",
    "\n",
    "optimizer = ThinkingBudgetOptimizer()\n",
    "\n",
    "print(\"Complexity Analysis:\\n\")\n",
    "for prompt in test_prompts:\n",
    "    complexity = optimizer.estimate_complexity(prompt)\n",
    "    budget = optimizer.get_optimal_budget(prompt, ClaudeModel.SONNET_4_5)\n",
    "    print(f\"Prompt: {prompt[:50]}...\")\n",
    "    print(f\"  Complexity: {complexity}\")\n",
    "    print(f\"  Recommended budget: {budget:,} tokens\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Patterns\n",
    "\n",
    "Advanced patterns for using extended thinking in production systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentThinkingInterface:\n",
    "    \"\"\"Interface for AI agents to use extended thinking.\"\"\"\n",
    "    \n",
    "    def __init__(self, client: anthropic.Anthropic):\n",
    "        self.client = client\n",
    "        self.optimizer = ThinkingBudgetOptimizer()\n",
    "        self.thinking_cache = {}  # Cache thinking for similar prompts\n",
    "        \n",
    "    def think_and_execute(\n",
    "        self,\n",
    "        task: str,\n",
    "        context: Optional[Dict[str, Any]] = None,\n",
    "        model: ClaudeModel = ClaudeModel.SONNET_4_5,\n",
    "        thinking_mode: str = \"auto\",  # auto, minimal, maximal\n",
    "        return_thinking: bool = False\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"Execute task with optimal thinking configuration.\"\"\"\n",
    "        \n",
    "        # Build prompt with context\n",
    "        messages = []\n",
    "        \n",
    "        if context:\n",
    "            system_context = f\"Context:\\n{json.dumps(context, indent=2)}\"\n",
    "            messages.append({\"role\": \"user\", \"content\": system_context})\n",
    "        \n",
    "        messages.append({\"role\": \"user\", \"content\": task})\n",
    "        \n",
    "        # Determine thinking budget\n",
    "        if thinking_mode == \"minimal\":\n",
    "            budget = 1024\n",
    "        elif thinking_mode == \"maximal\":\n",
    "            budget = MODEL_CAPABILITIES[model][\"max_thinking_budget\"]\n",
    "        else:  # auto\n",
    "            budget = self.optimizer.get_optimal_budget(task, model)\n",
    "        \n",
    "        # Check if model supports thinking\n",
    "        params = {\n",
    "            \"model\": model.value,\n",
    "            \"messages\": messages,\n",
    "            \"max_tokens\": 8000\n",
    "        }\n",
    "        \n",
    "        if MODEL_CAPABILITIES[model][\"extended_thinking\"] and budget > 0:\n",
    "            params[\"thinking\"] = {\n",
    "                \"type\": \"enabled\",\n",
    "                \"budget_tokens\": budget\n",
    "            }\n",
    "        \n",
    "        # Execute request\n",
    "        response = self.client.messages.create(**params)\n",
    "        \n",
    "        # Process response\n",
    "        result = {\n",
    "            \"success\": True,\n",
    "            \"answer\": \"\",\n",
    "            \"thinking\": [],\n",
    "            \"model\": model.name,\n",
    "            \"thinking_budget_used\": budget,\n",
    "            \"metadata\": {\n",
    "                \"complexity\": self.optimizer.estimate_complexity(task),\n",
    "                \"thinking_mode\": thinking_mode\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        for block in response.content:\n",
    "            if block.type == \"thinking\" and return_thinking:\n",
    "                result[\"thinking\"].append(block.thinking)\n",
    "            elif block.type == \"text\":\n",
    "                result[\"answer\"] += block.text\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def batch_think(\n",
    "        self,\n",
    "        tasks: List[str],\n",
    "        model: ClaudeModel = ClaudeModel.SONNET_4_5,\n",
    "        parallel: bool = False\n",
    "    ) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Process multiple tasks with thinking.\"\"\"\n",
    "        results = []\n",
    "        \n",
    "        for task in tasks:\n",
    "            result = self.think_and_execute(task, model=model)\n",
    "            results.append(result)\n",
    "            \n",
    "            # Add delay between requests to avoid rate limits\n",
    "            if not parallel:\n",
    "                import time\n",
    "                time.sleep(1)\n",
    "        \n",
    "        return results\n",
    "\n",
    "# Initialize agent interface\n",
    "agent_interface = AgentThinkingInterface(client)\n",
    "\n",
    "# Example usage for agents\n",
    "example_agent_task = {\n",
    "    \"task\": \"Analyze this code for potential bugs and suggest improvements\",\n",
    "    \"context\": {\n",
    "        \"code\": \"def calculate_average(numbers):\\n  return sum(numbers) / len(numbers)\",\n",
    "        \"language\": \"python\",\n",
    "        \"purpose\": \"Calculate mean of number list\"\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"Agent Interface Example:\")\n",
    "print(json.dumps(example_agent_task, indent=2))\n",
    "print(\"\\n[Ready for agent integration]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integration with AI Agents\n",
    "\n",
    "Example of how to integrate extended thinking into your AI agent system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TypeScript interface equivalent (for reference)\n",
    "typescript_interface = \"\"\"\n",
    "// TypeScript interface for extended thinking integration\n",
    "\n",
    "interface ExtendedThinkingConfig {\n",
    "  enabled: boolean;\n",
    "  budgetTokens: number;\n",
    "  mode: 'auto' | 'minimal' | 'moderate' | 'maximal';\n",
    "  returnThinking: boolean;\n",
    "}\n",
    "\n",
    "interface ThinkingResponse {\n",
    "  success: boolean;\n",
    "  answer: string;\n",
    "  thinking?: string[];\n",
    "  metadata: {\n",
    "    model: string;\n",
    "    thinkingBudgetUsed: number;\n",
    "    complexity: string;\n",
    "  };\n",
    "}\n",
    "\n",
    "class ExtendedThinkingService {\n",
    "  async executeWithThinking(\n",
    "    prompt: string,\n",
    "    config?: Partial<ExtendedThinkingConfig>\n",
    "  ): Promise<ThinkingResponse> {\n",
    "    // Implementation would call the Python service or API\n",
    "  }\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "# Agent integration example\n",
    "class SmartAgent:\n",
    "    \"\"\"Example agent using extended thinking.\"\"\"\n",
    "    \n",
    "    def __init__(self, name: str, thinking_interface: AgentThinkingInterface):\n",
    "        self.name = name\n",
    "        self.thinking = thinking_interface\n",
    "        \n",
    "    def solve_problem(self, problem: str, use_deep_thinking: bool = True) -> str:\n",
    "        \"\"\"Solve a problem using extended thinking.\"\"\"\n",
    "        \n",
    "        # Determine thinking mode based on problem\n",
    "        if use_deep_thinking:\n",
    "            mode = \"maximal\" if \"complex\" in problem.lower() else \"auto\"\n",
    "        else:\n",
    "            mode = \"minimal\"\n",
    "        \n",
    "        # Execute with thinking\n",
    "        result = self.thinking.think_and_execute(\n",
    "            task=problem,\n",
    "            thinking_mode=mode,\n",
    "            return_thinking=True  # Get thinking for logging\n",
    "        )\n",
    "        \n",
    "        # Log thinking process (useful for debugging)\n",
    "        if result[\"thinking\"]:\n",
    "            print(f\"[{self.name}] Used {len(result['thinking'])} thinking blocks\")\n",
    "            print(f\"[{self.name}] Complexity: {result['metadata']['complexity']}\")\n",
    "        \n",
    "        return result[\"answer\"]\n",
    "\n",
    "# Create example agents\n",
    "code_reviewer = SmartAgent(\"CodeReviewer\", agent_interface)\n",
    "test_generator = SmartAgent(\"TestGenerator\", agent_interface)\n",
    "bug_hunter = SmartAgent(\"BugHunter\", agent_interface)\n",
    "\n",
    "print(\"Smart Agents Created:\")\n",
    "print(f\"  - {code_reviewer.name}\")\n",
    "print(f\"  - {test_generator.name}\")\n",
    "print(f\"  - {bug_hunter.name}\")\n",
    "print(\"\\n[Agents ready with extended thinking capabilities]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error Handling\n",
    "\n",
    "Robust error handling for production systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExtendedThinkingError(Exception):\n",
    "    \"\"\"Base exception for extended thinking errors.\"\"\"\n",
    "    pass\n",
    "\n",
    "class ThinkingBudgetError(ExtendedThinkingError):\n",
    "    \"\"\"Error when thinking budget is invalid.\"\"\"\n",
    "    pass\n",
    "\n",
    "class ModelCapabilityError(ExtendedThinkingError):\n",
    "    \"\"\"Error when model doesn't support requested feature.\"\"\"\n",
    "    pass\n",
    "\n",
    "def safe_thinking_request(\n",
    "    client: anthropic.Anthropic,\n",
    "    model: ClaudeModel,\n",
    "    messages: List[Dict[str, str]],\n",
    "    thinking_budget: int = 2000,\n",
    "    max_retries: int = 3\n",
    ") -> Optional[Any]:\n",
    "    \"\"\"Make a safe thinking request with error handling and retries.\"\"\"\n",
    "    \n",
    "    # Validate model capabilities\n",
    "    if not MODEL_CAPABILITIES[model].get(\"extended_thinking\", False):\n",
    "        print(f\"‚ö†Ô∏è {model.name} doesn't support extended thinking, falling back to standard mode\")\n",
    "        return client.messages.create(\n",
    "            model=model.value,\n",
    "            messages=messages,\n",
    "            max_tokens=4000\n",
    "        )\n",
    "    \n",
    "    # Validate thinking budget\n",
    "    if thinking_budget < 1024:\n",
    "        raise ThinkingBudgetError(f\"Budget {thinking_budget} is below minimum (1024)\")\n",
    "    \n",
    "    max_budget = MODEL_CAPABILITIES[model][\"max_thinking_budget\"]\n",
    "    if thinking_budget > max_budget:\n",
    "        print(f\"‚ö†Ô∏è Budget {thinking_budget} exceeds max ({max_budget}), using max\")\n",
    "        thinking_budget = max_budget\n",
    "    \n",
    "    # Attempt request with retries\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            response = client.messages.create(\n",
    "                model=model.value,\n",
    "                messages=messages,\n",
    "                max_tokens=4000,\n",
    "                thinking={\n",
    "                    \"type\": \"enabled\",\n",
    "                    \"budget_tokens\": thinking_budget\n",
    "                }\n",
    "            )\n",
    "            return response\n",
    "            \n",
    "        except anthropic.RateLimitError as e:\n",
    "            print(f\"Rate limit hit, waiting {2 ** attempt} seconds...\")\n",
    "            import time\n",
    "            time.sleep(2 ** attempt)\n",
    "            \n",
    "        except anthropic.APIError as e:\n",
    "            if \"thinking\" in str(e) and \"temperature\" in str(e):\n",
    "                # Incompatible parameters, retry without temperature\n",
    "                print(\"Removing incompatible parameters...\")\n",
    "                return client.messages.create(\n",
    "                    model=model.value,\n",
    "                    messages=messages,\n",
    "                    max_tokens=4000,\n",
    "                    thinking={\n",
    "                        \"type\": \"enabled\",\n",
    "                        \"budget_tokens\": thinking_budget\n",
    "                    }\n",
    "                )\n",
    "            else:\n",
    "                print(f\"API Error on attempt {attempt + 1}: {e}\")\n",
    "                if attempt == max_retries - 1:\n",
    "                    raise\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"Unexpected error: {e}\")\n",
    "            if attempt == max_retries - 1:\n",
    "                raise\n",
    "    \n",
    "    return None\n",
    "\n",
    "# Test error handling\n",
    "print(\"Error Handling Examples:\\n\")\n",
    "\n",
    "# Example 1: Budget too small\n",
    "try:\n",
    "    safe_thinking_request(\n",
    "        client,\n",
    "        ClaudeModel.SONNET_4_5,\n",
    "        [{\"role\": \"user\", \"content\": \"test\"}],\n",
    "        thinking_budget=500\n",
    "    )\n",
    "except ThinkingBudgetError as e:\n",
    "    print(f\"‚úì Caught budget error: {e}\")\n",
    "\n",
    "# Example 2: Model without thinking support\n",
    "print(\"\\n‚úì Testing fallback for non-thinking model:\")\n",
    "# This would fall back to standard mode\n",
    "# safe_thinking_request(\n",
    "#     client,\n",
    "#     ClaudeModel.HAIKU_3,\n",
    "#     [{\"role\": \"user\", \"content\": \"test\"}]\n",
    "# )\n",
    "\n",
    "print(\"\\n[Error handling configured]\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}